lookup <- c("Y" = 1, "y" = 1, "1" = 1, "N" = 0, "n" = 0, "0" = 0)
sales$AFCurrentUseLand <- lookup[sales$AFCurrentUseLand]
sales$AFCurrentUseLand[is.na(sales$AFCurrentUseLand)] = 0
sales$AFForestLand <- lookup[sales$AFForestLand]
sales$AFForestLand[is.na(sales$AFForestLand)] = 0
sales$AFHistoricProperty <- lookup[sales$AFHistoricProperty]
sales$AFHistoricProperty[is.na(sales$AFHistoricProperty)] = 0
sales$AFNonProfitUse <-lookup[sales$AFNonProfitUse]
sales$AFNonProfitUse[is.na(sales$AFNonProfitUse)] = 0
#
final_df <- sales %>%
filter(SalePrice > 0) %>%
left_join(resBldg, by="mm_key") %>%
group_by(mm_key, DocumentDate) %>%
filter(DocumentDate >= YrBuilt) %>%
filter(YrBuilt == max(YrBuilt)) %>%
left_join(parcel, by = "mm_key") %>%
#left_join(c_units, by = "mm_key") %>%
relocate(SalePrice, .after = "mm_key")
train_pcr <- as.data.frame(cbind(Y_train, X_train))
names(train_pcr)[names(train_pcr) == 'Y_train'] <- 'SalePrice'
check_levels <- sapply(lapply(train_pcr, unique), length)
one_level <- as.data.frame(check_levels) %>%
filter(check_levels == 1)
train_pcr_2 <- train_pcr[, !(names(train_pcr) %in% row.names(one_level))]
pcr_mod <- pcr(SalePrice ~ ., data = train_pcr_2, scale = T, validation = "CV")
rm(data_1970)
rm(data_1982)
rm(data_1992)
rm(data_2002)
rm(data_2017)
rm(data_clean)
rm(sales_clean)
rm(mod_1970)
rm(mod)
rm(mod_1982)
rm(mod)
rm(mod_1992)
rm(mod_2002)
rm(mod_2017)
rm(data_raw)
rm(data_use)
rm(data_model)
rm(fertility_data)
rm(resBldg)
rm(more_than_one_level)
rm(condo_units)
rm(c_units)
rm(cleaned_condo_units)
rm(cu_df)
rm(d)
pcr_mod <- pcr(SalePrice ~ ., data = train_pcr_2, scale = T, validation = "CV")
pcr_mod <- pcr(SalePrice ~ ., data = train_pcr_2, scale = T, validation = "CV")
pcr_mod <- pcr(SalePrice ~ ., data = train_pcr_2, scale = T, validation = "CV")
set.seed(123)
rand <- sample(0.7*nrow(data))
X_train <- X[rand, ]
Y_train <- Y[rand]
X_test <- X[-rand, ]
Y_test <- Y[-rand]
# Model Selection
# check for variation in predictors
train_pcr <- as.data.frame(cbind(Y_train, X_train))
names(train_pcr)[names(train_pcr) == 'Y_train'] <- 'SalePrice'
check_levels <- sapply(lapply(train_pcr, unique), length)
one_level <- as.data.frame(check_levels) %>%
filter(check_levels == 1)
train_pcr_2 <- train_pcr[, !(names(train_pcr) %in% row.names(one_level))]
pcr_mod <- pcr(SalePrice ~ ., data = train_pcr_2, scale = T, validation = "CV")
set.seed(123)
rand <- sample(0.6*nrow(data))
X_train <- X[rand, ]
Y_train <- Y[rand]
X_test <- X[-rand, ]
Y_test <- Y[-rand]
# Model Selection
# check for variation in predictors
train_pcr <- as.data.frame(cbind(Y_train, X_train))
names(train_pcr)[names(train_pcr) == 'Y_train'] <- 'SalePrice'
check_levels <- sapply(lapply(train_pcr, unique), length)
one_level <- as.data.frame(check_levels) %>%
filter(check_levels == 1)
train_pcr_2 <- train_pcr[, !(names(train_pcr) %in% row.names(one_level))]
pcr_mod <- pcr(SalePrice ~ ., data = train_pcr_2, scale = T, validation = "CV")
knitr::opts_chunk$set(echo = TRUE)
library('ISLR2')
attach(Boston)
data <- Boston
View(data)
install.packages("xgboost")
library(xgboost)
library(dplyr)
# Check distribution of response and type of each variable
sapply(data, class)
# Assume chas and rad are factors and set to factors
data$rad <- as.factor(data$rad)
# Assume chas and rad are factors and set to factors
data$rad <- as.factor(data$rad)
data$chas <- as.factor(data$chas)
# Check distribution of variables
lapply(data, skew)
# Check distribution of variables
lapply(data, skew())
library(psych)
# Check distribution of variables
lapply(data, skew)
data <- Boston
# Check distribution of variables
lapply(data, skew)
data$rad <- as.factor(data$rad)
data$chas <- as.factor(data$chas)
data_norm <- data %>%
mutate(crim = log(crim),
zn = log(zn),
indus = log(indus),
nox = log(nox),
rm = log(rm),
age = log(age),
dis = log(dis),
tax = log(tax),
ptratio = log(ptratio),
medv = log(medv))
View(data_norm)
data_norm <- data %>%
mutate(crim = log(crim),
zn = log(zn + 1),
indus = log(indus),
nox = log(nox),
rm = log(rm),
age = log(age),
dis = log(dis),
tax = log(tax),
ptratio = log(ptratio),
medv = log(medv))
# Scale data
data_use <- scale(ddata_norm)
# Scale data
data_use <- scale(data_norm)
# Scale data
data_use <- data_norm %>%
mutate_if(is.numeric, scale)
test_index <- sample(1:nrow(data_use), as.integer(nrow(data_use)*0.2))
train_set <- final_df[-test_index,]
# Split train and test set
test_index <- sample(1:nrow(data_use), as.integer(nrow(data_use)*0.2))
train_set <- data_use[-test_index,]
test_set <- data_use[test_index,]
train_x <- as.matrix(train_set[, names(train_set) != "medv"])
train_x <- as.matrix(train_set[, names(train_set) != "medv"])
train_y <- train_set$medv
test_x <- as.matrix(test_set[, names(test_set) != "medv"])
test_y <- test_set$medv
xgbTrain <- xgb.DMatrix(data=train_x, label=train_y)
# Make factors numeric otherwise model doesn't run
data_use$rad <- as.numeric(data_use$rad)
data_use$chas <- as.numeric(data_use$chas)
# Split train and test set
test_index <- sample(1:nrow(data_use), as.integer(nrow(data_use)*0.2))
train_set <- data_use[-test_index,]
test_set <- data_use[test_index,]
train_x <- as.matrix(train_set[, names(train_set) != "medv"])
train_y <- train_set$medv
test_x <- as.matrix(test_set[, names(test_set) != "medv"])
test_y <- test_set$medv
xgbTrain <- xgb.DMatrix(data=train_x, label=train_y)
xgbTest <- xgb.DMatrix(data=test_x)
params <- list(grow_policy="lossguide")
mod1 <- xgb.train(data = xgbTrain, nrounds = 500, verbose = 2, grow_policy="lossguide", print_every_n = 10)
xgbPred = predict(mod1, xgbTest)
resi = validation_y - xgbPred
xgMSE = mean(exp(test_y) - exp(xgbPred))^2
y_mean = mean(exp(test_y))
library('ISLR2')
library(xgboost)
library(psych)
library(dplyr)
attach(Boston)
data <- Boston
# Check distribution of response and type of each variable
lapply(data, skew)
sapply(data, class)
# Normalize skews greater than 0.7 or less than -0.7 exclude factors
data_norm <- data %>%
mutate(crim = log(crim),
zn = log(zn + 1),
indus = log(indus),
nox = log(nox),
rm = log(rm),
age = log(age),
dis = log(dis),
tax = log(tax),
ptratio = log(ptratio),
medv = log(medv))
# Assume chas and rad are factors and set to factors
data_norm$rad <- as.factor(data$rad)
data_norm$chas <- as.factor(data$chas)
# Scale data
data_use <- data_norm %>%
mutate_if(is.numeric, scale)
# Make factors numeric otherwise model doesn't run
data_use$rad <- as.numeric(data_use$rad)
data_use$chas <- as.numeric(data_use$chas)
# Split train and test set
test_index <- sample(1:nrow(data_use), as.integer(nrow(data_use)*0.2))
train_set <- data_use[-test_index,]
test_set <- data_use[test_index,]
train_x <- as.matrix(train_set[, names(train_set) != "medv"])
train_y <- train_set$medv
test_x <- as.matrix(test_set[, names(test_set) != "medv"])
test_y <- test_set$medv
xgbTrain <- xgb.DMatrix(data=train_x, label=train_y)
xgbTest <- xgb.DMatrix(data=test_x)
params <- list(grow_policy="lossguide")
mod1 <- xgb.train(data = xgbTrain, nrounds = 500, verbose = 2, grow_policy="lossguide", print_every_n = 10)
xgbPred = predict(mod1, xgbTest)
resi = validation_y - xgbPred
xgbPred = predict(mod1, xgbTest)
resi = validation_y - xgbPred
resi = test_y - xgbPred
xgMSE = mean(exp(test_y) - exp(xgbPred))^2
y_mean = mean(exp(test_y))
tss = sum((exp(test_y) - exp(y_mean))^2)
rss = sum(resi^2)
xgbR2 = 1 - (rss/tss)
xgbRmse
xgbMSE
xgMSE
summary(mod1)
fn <- as.formula(~ -1 + rm + rm:(zn + indus + chas + nox + crim + age + dis + rad + tax + ptratio + lstat) +
(zn + indus + chas + nox + crim + age + dis + rad + tax + ptratio + lstat) ^ 2)
# Interpretable model - double LASSO
library(hdm)
X <- model.matrix(fn, data = Boston)
X <- X[, which(apply(X, 2, var) != 0)]
index_rm <- grep("rm", colnames(X))
y <- Boston$medv
mod <- lm(y ~ X)
rm_effect <- rlassoEffects(x = X, y = y, index = index_rm)
rm_effect <- rlassoEffects(x = X, y = y, index = index_rm)
summary(crime_effe
summary(rm_effect)
library(class)
# Read in data
data <- as.matrix(data)
X <- data[, 1:12]
Y <- data[, 13]
# Split into train and test set
set.seed(123)
rand <- sample(0.8*nrow(data))
X_train <- X[rand, ]
Y_train <- Y[rand]
X_test <- X[-rand, ]
Y_test <- Y[-rand]
# Select k = sqrt(n)
mod <- knn(X_train, X_test, Y_train, k=sqrt(nrow(Y_train)))
# Select k = sqrt(n)
mod <- knn(train_x, test_x, train_y, k=sqrt(nrow(train_set)))
# Inspect Model
table(exp(pred), exp(test_y))
summary(mod)
# Inspect Model
table(pred, test_y)
# Select k = sqrt(n)
mod <- knn(train_x, test_x, train_y, k=sqrt(nrow(train_set)))
# Inspect Model
table(pred, test_y)
mean(exp(knn_pred) == exp(Y_test))
# Select k = sqrt(n)
knn_pred <- knn(train_x, test_x, train_y, k=sqrt(nrow(train_set)))
mean(exp(knn_pred) == exp(Y_test))
mean(knn_pred == test_y)
# Inspect Model
table(knn_pred, test_y)
library(class)
# Select k = sqrt(n)
knn_pred <- knn(train_x, test_x, train_y, k=sqrt(nrow(train_set)))
# Inspect Model
#table(knn_pred, test_y)
knn_mse <- mean(knn_pred == test_y)
knn_mse
summary(rm_effect)[, 1]
summary(rm_effect)
rm_effect$coefficients
rm_effect$coefficients[rm_effect$coefficients]
rm_effect$coefficients[, 2]
rm_effect$coefficients
summary(rm_effect)$coefficients
summary(rm_effect)$coefficients[, 1]
mean(rm_effect$coefficients)
rm_mean <- mean(data$rm)
summary(rm_effect)
mean(rm_effect$coefficients)
summary(rm_effect)
View(data)
mean(rm_effect$coefficients)
sum(rm_effect$coefficients)
# Inspect Model
table(knn_pred, test_y)
library(caret)
shap_long <- shap.prep(mod1, X_train=train_x)
install.packages("SHAPforxgboost")
library(SHAPforxgboost)
shap_long <- shap.prep(mod1, X_train=train_x)
shap.plot.dependence(shap_long, x='rm', color_feature = "auto")
shap.plot.dependence(shap_long, x="rm", color_feature = "auto")
shap.plot.dependence(shap_long, x="Istat", color_feature = "auto")
shap.plot.dependence(shap_long, x="rm", color_feature = "auto")
shap.plot.dependence(shap_long, x="lstat", color_feature = "auto")
shap.plot.dependence(shap_long, x="rm", color_feature = "auto")
shap.plot.summary.wrap1(mod1, X = test_x, top_n = 3)
shap.plot.dependence(shap_long, x="rm", color_feature = "auto")
# Worst performance - OLS
ols.fit <- lm(medv ~ ., data = train_set)
ols.pred <- predict(ols.fit, test_x)
ols.pred <- predict(ols.fit, as.data.frame(test_x))
library('ISLR2')
library(xgboost)
library(psych)
library(dplyr)
attach(Boston)
data <- Boston
# Check distribution of response and type of each variable
lapply(data, skew)
sapply(data, class)
# Normalize skews greater than 0.7 or less than -0.7 exclude factors
data_norm <- data %>%
mutate(crim = log(crim),
zn = log(zn + 1),
indus = log(indus),
nox = log(nox),
rm = log(rm),
age = log(age),
dis = log(dis),
tax = log(tax),
ptratio = log(ptratio),
medv = log(medv))
# Assume chas and rad are factors and set to factors
data_norm$rad <- as.factor(data$rad)
data_norm$chas <- as.factor(data$chas)
# Scale data
data_use <- data_norm %>%
mutate_if(is.numeric, scale)
# Make factors numeric otherwise model doesn't run
data_use$rad <- as.numeric(data_use$rad)
data_use$chas <- as.numeric(data_use$chas)
# Split train and test set
test_index <- sample(1:nrow(data_use), as.integer(nrow(data_use)*0.2))
train_set <- data_use[-test_index,]
test_set <- data_use[test_index,]
train_x <- as.matrix(train_set[, names(train_set) != "medv"])
train_y <- train_set$medv
test_x <- as.matrix(test_set[, names(test_set) != "medv"])
test_y <- test_set$medv
# Boosted Regression - low interpretability
xgbTrain <- xgb.DMatrix(data=train_x, label=train_y)
xgbTest <- xgb.DMatrix(data=test_x)
params <- list(grow_policy="lossguide")
mod1 <- xgb.train(data = xgbTrain, nrounds = 500, verbose = 2, grow_policy="lossguide", print_every_n = 10)
xgbPred = predict(mod1, xgbTest)
resi = test_y - xgbPred
xgMSE = mean(exp(test_y) - exp(xgbPred))^2
y_mean = mean(exp(test_y))
tss = sum((exp(test_y) - exp(y_mean))^2)
rss = sum(resi^2)
xgbR2 = 1 - (rss/tss)
xgMSE
summary(mod1)
library(SHAPforxgboost)
shap_long <- shap.prep(mod1, X_train=train_x)
shap.plot.summary.wrap1(mod1, X = test_x, top_n = 3)
shap.plot.dependence(shap_long, x="rm", color_feature = "auto")
# Interpretable model - double LASSO
library(hdm)
fn <- as.formula(~ -1 + rm + rm:(zn + indus + chas + nox + crim + age + dis + rad + tax + ptratio + lstat) +
(zn + indus + chas + nox + crim + age + dis + rad + tax + ptratio + lstat) ^ 2)
X <- model.matrix(fn, data = Boston)
X <- X[, which(apply(X, 2, var) != 0)]
index_rm <- grep("rm", colnames(X))
y <- Boston$medv
mod <- lm(y ~ X)
rm_effect <- rlassoEffects(x = X, y = y, index = index_rm)
summary(rm_effect)
sum(rm_effect$coefficients)
library(class)
# Select k = sqrt(n)
knn_pred <- knn(train_x, test_x, train_y, k=sqrt(nrow(train_set)))
# Inspect Model
table(knn_pred, test_y)
knn_mse <- mean(knn_pred == test_y)
knn_mse
library(class)
# Select k = sqrt(n)
knn_pred <- knn(train_x, test_x, train_y, k=sqrt(nrow(train_set)))
# Inspect Model
#table(knn_pred, test_y)
knn_mse <- mean(knn_pred == test_y)
knn_mse
# Worst performance - OLS
ols.fit <- lm(medv ~ ., data = train_set)
ols.pred <- predict(ols.fit, as.data.frame(test_x))
ols.pred <- predict(ols.fit, test_x)
ols.pred <- predict(ols.fit, test_set)
ols.pred <- predict(ols.fit, test_set[, names(test_set) != "medv"])
summarise(ols.fit)
ols.pred <- predict(ols.fit, test_set[, names(test_set) != "medv"])
summary(ols.fit)
mse.ols <- mean((ols.pred - test_y)^2)
mse.ols
xgMSE
summary(ols.fit)
# Best performance - Boosted Regression previously created
shap.plot.summary.wrap1(mod1, X = test_x, top_n = 3)
shap.plot.dependence(shap_long, x="lstat", color_feature = "auto")
knn.mod <- knnreg(train_x, train_y)
summary(knn.mod)
pred.knn <- predict(knn.mod, test_x)
# Inspect Model
#table(knn_pred, test_y)
knn_mse <- mean((pred.knn == test_y)^2)
knn_mse
knn.mod <- knnreg(train_x, train_y)
pred.knn <- predict(knn.mod, test_x)
summary(knn.mod)
# Inspect Model
#table(knn_pred, test_y)
knn_mse <- mean((pred.knn == test_y)^2)
knn_mse
summary(knn.mod)
# Select k = sqrt(n)
knn_pred <- knn(train_x, test_x, train_y, k=sqrt(nrow(train_set)))
summary(knn_pred)
# Inspect Model
#table(knn_pred, test_y)
knn_mse <- mean((knn_pred == test_y)^2)
knn_mse
# Inspect Model
table(knn_pred, rm)
# Select k = sqrt(n)
knn.mod <- knnreg(medv ~., data = train_set)
summary(knn.mod)
knn.mod
summary(knn.mod)
plot(knn.mod)
setwd("~Desktop/DATASCI200")
rnorm(n = 10, mean = 2, sd = 1)
short_simulation <-  c(1,2,3,2,4,2)
true_density <- function(x) {
dnorm(x = x, mean = 2, sd = 1)
}
dat_hist <- data.frame(short_simulation)
dat_hist %>%
ggplot() +
geom_histogram(
aes(x = short_simulation, y = ..density..)) +
stat_function(
aes(x = short_simulation), fun = true_density, color = 'darkred')
library(tidyverse)
theme_set(theme_minimal())
d <- data.frame(
x = seq(from = 0, to = 4, by = 0.01)) %>%
mutate(
y = dnorm(x, mean = 2, sd = 1)
)
d %>%
ggplot() +
aes(x = x, y = y) +
geom_line()
d
x = rnorm(n =5, 2, 1)
hist(x)
x = rnorm(n =15, 2, 1)
hist(x)
x = rnorm(n =100, 2, 1)
hist(x)
x = rnorm(n =1000, 2, 1)
hist(x)
x = rnorm(n =10000, 2, 1)
hist(x)
simulation <- replicate(
n    = 10,      # should you change this line?
expr = 1 + .2   # or this line?
)
simulation
rz <- function() {
X = rnorm(n =10000, 2, 1)
Y = rnorm(n =10000, 2, 1)
Z = X + Y
}
rz <- function() {
X = rnorm(n =10000, 2, 1)
Y = rnorm(n =10000, 2, 1)
Z = X + Y
Z
}
Z
X = rnorm(n =10000, 2, 1)
Y = rnorm(n =10000, 2, 1)
Z = X + Y
Z
hist(Z)
mean(Z)
sd(Z)
var(Z)
res <- NA
for(i in 1:1000) {
res[i] <- runif(n=10, min=21, max=101) |>
mean()
}
res
shapiro.test(res)
setwd("~/Desktop/University/paintk2.github.io")
install.packages("rstudio_viewer")
